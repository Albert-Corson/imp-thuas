{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b28088a3",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "TODO\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746a805",
   "metadata": {
    "scrolled": false,
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl > /dev/null 2>&1\n",
    "!pip install jupyterlab-widgets > /dev/null 2>&1\n",
    "!pip install jsfileupload > /dev/null 2>&1\n",
    "!pip install pyxlsb > /dev/null 2>&1\n",
    "!pip install sklearn > /dev/null 2>&1\n",
    "!pip install scipy > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2801",
   "metadata": {},
   "source": [
    "## Set an arbitrary random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2deec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e92bd3",
   "metadata": {},
   "source": [
    "## Run the pre-imputation script\n",
    "\n",
    "AKA load the data, truncate it and create gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8057fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run helpers/pre_imputation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers/create_gaps.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ed199",
   "metadata": {},
   "source": [
    "## Define the hotdeck imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e907ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Config\n",
    "surrounding_duration = timedelta(days=15)\n",
    "\n",
    "# Internal globals\n",
    "cached_donors = None\n",
    "donors = None\n",
    "gaps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the helpers\n",
    "%run helpers/hotdeck.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47757cbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hotdeck(receiver: pd.DataFrame, config: object) -> pd.DataFrame:\n",
    "    global file_select, donors, gaps_done\n",
    "\n",
    "    custom_progress_status = widgets.HTML()\n",
    "    display(custom_progress_status)\n",
    "\n",
    "    if cached_donors == None:\n",
    "        load_donors_in_cache(donors, custom_progress_status)\n",
    "\n",
    "    gaps_done = 0\n",
    "    receiver_cp = receiver.copy()\n",
    "    gap_indices = config[\"current_gap_indices\"]\n",
    "\n",
    "    ui_thread = threading.Thread(target=run_ui, args=(len(gap_indices), custom_progress_status))\n",
    "    ui_thread.start()\n",
    "    impute(receiver_cp, gap_indices)\n",
    "    ui_thread.join()\n",
    "\n",
    "    return receiver_cp\n",
    "\n",
    "\n",
    "def impute(receiver: pd.DataFrame, gap_indices: [[datetime]]) -> None:\n",
    "    global surrounding_duration, column_name, donors, gaps_done\n",
    "\n",
    "    for gap in gap_indices:\n",
    "        gap_start_idx, gap_end_idx = get_gap_boundaries(receiver, gap[0], gap[-1])\n",
    "        gap_start_time = receiver.index[gap_start_idx]\n",
    "        gap_end_time = receiver.index[gap_end_idx]\n",
    "\n",
    "        duration_before, duration_after = get_sampling_durations(receiver, gap_start_idx, gap_end_idx, gap_start_time, gap_end_time)\n",
    "\n",
    "        before = get_normalized_dataframe(receiver, gap_start_time - duration_before, gap_start_time)\n",
    "        after = get_normalized_dataframe(receiver, gap_end_time, gap_end_time + duration_after)\n",
    "\n",
    "        donor_start_time = gap_start_time - (duration_before + surrounding_duration)\n",
    "        donor_end_time = gap_end_time + (duration_after + surrounding_duration)\n",
    "\n",
    "        scoreboard = []\n",
    "        for file in donors:\n",
    "            donor = get_donor(file, donor_start_time, donor_end_time)\n",
    "            if len(donor.index) != 0:\n",
    "                scoreboard.append(scan_donor(before.copy(), after.copy(), file, donor))\n",
    "        if len(scoreboard) != 0:\n",
    "            scoreboard.sort(key=lambda it: it[\"score\"])\n",
    "            fill_gap(scoreboard[0], receiver, gap)\n",
    "        gaps_done += 1\n",
    "\n",
    "    receiver.interpolate(method=\"index\", inplace=True)\n",
    "\n",
    "\n",
    "def scan_donor(before: pd.DataFrame, after: pd.DataFrame, donor_filename: str, donor: pd.DataFrame) -> [dict]:\n",
    "    global column_name\n",
    "\n",
    "    keys = np.concatenate([before[column_name].values, after[column_name].values])\n",
    "\n",
    "    gap_size = after.index[0] - before.index[-1]\n",
    "    before_size = before.index[-1] - before.index[0]\n",
    "    after_size = after.index[-1] - after.index[0]\n",
    "\n",
    "    donor_before = get_normalized_dataframe(donor, donor.index[0], donor.index[-1] - (after_size + gap_size))\n",
    "    donor_after = get_normalized_dataframe(donor, donor.index[0] + (gap_size + before_size), donor.index[-1])\n",
    "\n",
    "    sliding_before = sliding_window_view(donor_before[column_name].values, window_shape=len(before.index))\n",
    "    sliding_after = sliding_window_view(donor_after[column_name].values, window_shape=len(after.index))\n",
    "\n",
    "    length = min(len(sliding_before), len(sliding_after))\n",
    "    matrix = np.concatenate([sliding_before[:length], sliding_after[:length]], axis=1)\n",
    "\n",
    "#     matrix, y_offsets, ratios = compare_scale(keys, matrix)\n",
    "    matrix, y_offsets = compare_diff(keys, matrix)\n",
    "\n",
    "    matrix -= keys\n",
    "    matrix = np.absolute(matrix)\n",
    "\n",
    "    comp = matrix.sum(axis=1)\n",
    "    best = np.argsort(comp)[0]\n",
    "\n",
    "    return {\n",
    "        \"score\": comp[best],\n",
    "        \"x_offset\": before.index[0] - donor.index[best],\n",
    "        \"y_offset\": y_offsets[best],\n",
    "        \"ratio\": 1, #ratios[best],\n",
    "        \"start\": donor.index[best],\n",
    "        \"end\": donor.index[best] + before_size + after_size + gap_size,\n",
    "        \"filename\": donor_filename\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_diff(keys, matrix):\n",
    "    y_offsets = keys.mean() - matrix.mean(axis=1)\n",
    "    matrix = np.array([matrix[i] + y_offsets[i] for i in range(len(y_offsets))])\n",
    "    return matrix, y_offsets\n",
    "\n",
    "\n",
    "def compare_scale(keys, matrix):\n",
    "    # Scale it\n",
    "    keys_amp = abs(keys.max() - keys.min())\n",
    "    mins = matrix.min(axis=1)\n",
    "    maxs = matrix.max(axis=1)\n",
    "    mamps = np.absolute(maxs - mins)\n",
    "    ratios = np.ones((len(mamps)))\n",
    "    ratios = np.divide(keys_amp, mamps, out=ratios, where=mamps != 0)\n",
    "    matrix = np.array([matrix[i] * ratios[i] for i in range(len(ratios))])\n",
    "    # Shift it by the mean\n",
    "    matrix, y_offsets = compare_diff(keys, matrix)\n",
    "    return matrix, y_offsets, ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da8809",
   "metadata": {},
   "source": [
    "## Run the imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78f2ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imputation_status = widgets.HTML(value=\"\")\n",
    "display(imputation_status)\n",
    "\n",
    "imputed_dfs = []\n",
    "\n",
    "\n",
    "def do_imputation():\n",
    "    global imputed_dfs\n",
    "    for i in range(len(dfs_with_gaps)):\n",
    "        imputation_status.value = f\"Running imputation... ({i}/{len(dfs_with_gaps)})\"\n",
    "        dataset_config['current_gap_indices'] = gaps_indices[i]\n",
    "        result = hotdeck(dfs_with_gaps[i], dataset_config)\n",
    "        imputed_dfs.append(result)\n",
    "    imputation_status.value = \"Imputation complete\"\n",
    "\n",
    "\n",
    "hotdeck_prehook(do_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ebae4",
   "metadata": {},
   "source": [
    "## Run the post imputation script\n",
    "\n",
    "AKA visualize and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fe795",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run helpers/evaluate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b3ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
